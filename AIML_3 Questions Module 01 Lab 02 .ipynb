{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNi5IKM+NJSVe78fIWW1asN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Allekarthik/AIML_Projects_and_labs/blob/main/AIML_3%20Questions%20Module%2001%20Lab%2002%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it? can u explain this question"
      ],
      "metadata": {
        "id": "1GvsLs335Zqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng(seed=42)"
      ],
      "metadata": {
        "id": "XlYlJVdK5gEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.fetch_california_housing()\n",
        "# Dataset description\n",
        "print(dataset.DESCR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yEgyBlo4h9f",
        "outputId": "c7fe6b7c-ccdd-4eeb-e572-a8a811b1c348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _california_housing_dataset:\n",
            "\n",
            "California Housing dataset\n",
            "--------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 20640\n",
            "\n",
            "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
            "\n",
            "    :Attribute Information:\n",
            "        - MedInc        median income in block group\n",
            "        - HouseAge      median house age in block group\n",
            "        - AveRooms      average number of rooms per household\n",
            "        - AveBedrms     average number of bedrooms per household\n",
            "        - Population    block group population\n",
            "        - AveOccup      average number of household members\n",
            "        - Latitude      block group latitude\n",
            "        - Longitude     block group longitude\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "This dataset was obtained from the StatLib repository.\n",
            "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
            "\n",
            "The target variable is the median house value for California districts,\n",
            "expressed in hundreds of thousands of dollars ($100,000).\n",
            "\n",
            "This dataset was derived from the 1990 U.S. census, using one row per census\n",
            "block group. A block group is the smallest geographical unit for which the U.S.\n",
            "Census Bureau publishes sample data (a block group typically has a population\n",
            "of 600 to 3,000 people).\n",
            "\n",
            "A household is a group of people residing within a home. Since the average\n",
            "number of rooms and bedrooms in this dataset are provided per household, these\n",
            "columns may take surprisingly large values for block groups with few households\n",
            "and many empty houses, such as vacation resorts.\n",
            "\n",
            "It can be downloaded/loaded using the\n",
            ":func:`sklearn.datasets.fetch_california_housing` function.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
            "      Statistics and Probability Letters, 33 (1997) 291-297\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Orignal target values:\", dataset.target)\n",
        "\n",
        "dataset.target = dataset.target.astype(int)\n",
        "\n",
        "print(\"Target values after conversion:\", dataset.target)\n",
        "print(\"Input variables shape:\", dataset.data.shape)\n",
        "print(\"Output variables shape:\", dataset.target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2nmw5Tc4h_9",
        "outputId": "d5f3e642-289d-42be-e0f7-c9bf0b363355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orignal target values: [4.526 3.585 3.521 ... 0.923 0.847 0.894]\n",
            "Target values after conversion: [4 3 3 ... 0 0 0]\n",
            "Input variables shape: (20640, 8)\n",
            "Output variables shape: (20640,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "\n",
        "    diff = (\n",
        "        traindata - query\n",
        "    )\n",
        "    sq = diff * diff\n",
        "    dist = sq.sum(1)\n",
        "    label = trainlabel[np.argmin(dist)]\n",
        "    return label\n",
        "\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "\n",
        "    predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "    return predlabel"
      ],
      "metadata": {
        "id": "whYEQOge4iCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "\n",
        "\n",
        "    classes = np.unique(trainlabel)\n",
        "    rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "    predlabel = classes[rints]\n",
        "    return predlabel"
      ],
      "metadata": {
        "id": "XEekqwO44iEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "\n",
        "    assert len(gtlabel) == len(\n",
        "        predlabel\n",
        "    ), \"Length of the ground-truth labels and predicted labels should be the same\"\n",
        "    correct = (\n",
        "        gtlabel == predlabel\n",
        "    ).sum()  # count the number of times the groundtruth label is equal to the predicted label.\n",
        "    return correct / len(gtlabel)"
      ],
      "metadata": {
        "id": "KwqBINgj4iF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split(data, label, percent):\n",
        "    # generate a random number for each sample\n",
        "    rnd = rng.random(len(label))\n",
        "    split1 = rnd < percent\n",
        "    split2 = rnd >= percent\n",
        "\n",
        "    split1data = data[split1, :]\n",
        "    split1label = label[split1]\n",
        "    split2data = data[split2, :]\n",
        "    split2label = label[split2]\n",
        "    return split1data, split1label, split2data, split2label"
      ],
      "metadata": {
        "id": "CYU3UhhP4iIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(\n",
        "    dataset.data, dataset.target, 20 / 100\n",
        ")\n",
        "print(\"Number of test samples:\", len(testlabel))\n",
        "print(\"Number of train samples:\", len(alltrainlabel))\n",
        "print(\"Percent of test data:\", len(testlabel) * 100 / len(dataset.target), \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Cp3JJXz4iJ1",
        "outputId": "bb5f4f3d-4597-48c8-d02c-abc9b20b4ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples: 4144\n",
            "Number of train samples: 16496\n",
            "Percent of test data: 20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# traindata, trainlabel, valdata, vallabel = split(\n",
        "#     alltraindata, alltrainlabel, 75 / 100)\n",
        "# #here 75/100 indicates 75% of the data will be used for training and the remaining 25% will be used for validation."
      ],
      "metadata": {
        "id": "MYaYycgQ4iLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# traindata, trainlabel, valdata, vallabel = split(\n",
        "#     alltraindata, alltrainlabel, 40 / 100)\n",
        "# #now i increased validation to 60% and trainging will be 40%\n",
        "# #  increasing the validation set size can lead to more reliable\n",
        "# # validation accuracy but might cause the model to be undertrained\n"
      ],
      "metadata": {
        "id": "pJHFVzWm6EH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(\n",
        "    alltraindata, alltrainlabel, 90 / 100)\n",
        "# here training is 90% and validation is 10%\n",
        "# Reducing the validation set size can lead to a\n",
        "#  better-trained model but might result in a less reliable and more variable\n",
        "#   validation accuracy."
      ],
      "metadata": {
        "id": "wWD6VO-R7Cid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PaH58E7k7ntr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using nearest neighbour algorithm:\", trainAccuracy*100, \"%\")\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using random classifier: \", trainAccuracy*100, \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrJSTi0q4iNl",
        "outputId": "036354b9-65b5-45f8-d519-2fe15a650b46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy using nearest neighbour algorithm: 100.0 %\n",
            "Training accuracy using random classifier:  17.210962224766007 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour algorithm:\", valAccuracy*100, \"%\")\n",
        "\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier:\", valAccuracy*100, \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cedTUsPO4iRL",
        "outputId": "eb0c136a-2b8e-473f-81ff-f0f3db8aae5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour algorithm: 33.44415898443403 %\n",
            "Validation accuracy using random classifier: 16.941212029620676 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wsPvQ89_6lgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?"
      ],
      "metadata": {
        "id": "hbuZsigV7pEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qhzRijc47yWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Set Size\n",
        "Larger Training Set:\n",
        "\n",
        "Better Learning: A larger training set provides more data for the model to learn from, which can improve the model's ability to generalize to new data.\n",
        "Reduced Overfitting: With more data, the model is less likely to overfit, as it can learn a more robust representation of the underlying patterns in the data.\n",
        "Smaller Training Set:\n",
        "\n",
        "Limited Learning: A smaller training set might not provide enough examples for the model to learn effectively, leading to poor generalization.\n",
        "Higher Risk of Overfitting: With less data, the model might overfit to the training examples, capturing noise rather than the actual patterns."
      ],
      "metadata": {
        "id": "NNQ-i3X77yy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation Set Size\n",
        "Larger Validation Set:\n",
        "\n",
        "More Representative Performance Estimate: A larger validation set is more likely to accurately reflect the model's performance on unseen data, giving a more reliable estimate of test set accuracy.\n",
        "Stable Accuracy Measurement: The validation accuracy is less prone to variability and statistical noise, providing a more stable measure of performance.\n",
        "Smaller Validation Set:\n",
        "\n",
        "Less Reliable Performance Estimate: A smaller validation set may not adequately represent the diversity of the data, leading to less reliable performance estimates.\n",
        "High Variability: The validation accuracy may fluctuate more due to the smaller sample size, making it harder to predict the test set performance accurately."
      ],
      "metadata": {
        "id": "hpOdisyH7-bY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?"
      ],
      "metadata": {
        "id": "7b586Ky78ed2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimal split ratio depends on various factors. The rough standard for train-validation-test splits is 60-80% training data, 10-20% validation data, and 10-20% test data."
      ],
      "metadata": {
        "id": "k4vbIrEv8em3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions on Multiple splits"
      ],
      "metadata": {
        "id": "JSrXE6cvFs3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)Does averaging the validation accuracy across multiple splits give more consistent results?"
      ],
      "metadata": {
        "id": "kFJtKG9yFs5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, averaging the validation accuracy across multiple splits typically provides more consistent and reliable results compared to using a single train-validation split"
      ],
      "metadata": {
        "id": "CfNYQzEtFs7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q)Does it give more accurate estimate of test accuracy?"
      ],
      "metadata": {
        "id": "fJAIqlEaFs9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While cross-validation provides valuable insights into model performance and generalization on training data, it does not directly estimate test accuracy. Test accuracy should always be evaluated on a separate, unseen test set to ensure an unbiased and realistic assessment of the model's performance in real-world applications. Cross-validation helps in ensuring that the model is well-generalized and robust based on the training data, facilitating better decisions in model selection and tuning."
      ],
      "metadata": {
        "id": "qvZ4ENr4Fs_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q)What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?"
      ],
      "metadata": {
        "id": "Bdmbkay6FtBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of iterations in cross-validation affects the reliability and stability of the estimate of model performance. Generally, increasing the number of iterations can lead to a more robust estimate, but the improvement diminishes with higher numbers of iterations"
      ],
      "metadata": {
        "id": "lzwsyS_gFtE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q)Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?"
      ],
      "metadata": {
        "id": "eKopwv_cFtG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing the number of iterations in cross-validation can indeed help in dealing with the challenges posed by very small training or validation datasets. It improves the robustness and stability of performance estimates, providing a more reliable assessment of how well your model generalizes to unseen data. However, itâ€™s essential to balance the benefits of increased iterations with practical considerations such as computational cost and dataset representativeness."
      ],
      "metadata": {
        "id": "YbN7lm58FtJH"
      }
    }
  ]
}